{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M-VIwlTx5IlF"
   },
   "source": [
    "# Multiclass Semantic Segmentation using CamVid dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4LiKXymP5IlH"
   },
   "source": [
    "## Introduction\n",
    "- Semantic Segmentation of an image is to assign each pixel in the input image a semantic class in order to get a pixel-wise dense classification.\n",
    "- A general semantic segmentation architecture can be broadly thought of as an encoder network followed by a decoder network. - - The encoder is usually is a pre-trained classification network like VGG/ResNet followed by a decoder network. The decoder network/mechanism is mostly where these architectures differ. \n",
    "- The task of the decoder is to semantically project the discriminative features (lower resolution) learnt by the encoder onto the pixel space (higher resolution) to get a dense classification. (mechanisms like skip connections, pyramid pooling etc are used for this)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PoaXg0w55IlI"
   },
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nfVmO7QC5IlK"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mirap/anaconda3/envs/alpha/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/mirap/anaconda3/envs/alpha/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/mirap/anaconda3/envs/alpha/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/mirap/anaconda3/envs/alpha/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/mirap/anaconda3/envs/alpha/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/mirap/anaconda3/envs/alpha/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/mirap/anaconda3/envs/alpha/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/mirap/anaconda3/envs/alpha/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/mirap/anaconda3/envs/alpha/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/mirap/anaconda3/envs/alpha/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/mirap/anaconda3/envs/alpha/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/mirap/anaconda3/envs/alpha/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import *\n",
    "import os\n",
    "import sys\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "\n",
    "#from tensorflow.keras.engine import Layer\n",
    "from tensorflow.keras.applications.vgg16 import *\n",
    "from tensorflow.keras.models import *\n",
    "#from tensorflow.keras.applications.imagenet_utils import _obtain_input_shape\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Convolution2D, ZeroPadding2D, MaxPooling2D, Cropping2D, Conv2D\n",
    "from tensorflow.keras.layers import Input, Add, Dropout, Permute, add\n",
    "from tensorflow.compat.v1.layers import conv2d_transpose\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.python.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aFSE2xhC5IlR"
   },
   "source": [
    "# Environment checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WIvQZjCe5IlS",
    "outputId": "d11a0c93-1546-440e-eb8f-85726f110a60",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "tf.enable_eager_execution()\n",
    "print(tf.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "sgQZswXp5IlX",
    "outputId": "4d3ce0f8-3849-49d3-e08b-f630c9fc0ed6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is there a GPU available: \n",
      "True\n",
      "Is the Tensor on GPU #0:  \n",
      "True\n",
      "Device name: /job:localhost/replica:0/task:0/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "x = tf.random.uniform([3, 3])\n",
    "\n",
    "print(\"Is there a GPU available: \"),\n",
    "print(tf.test.is_gpu_available())\n",
    "\n",
    "print(\"Is the Tensor on GPU #0:  \"),\n",
    "print(x.device.endswith('GPU:0'))\n",
    "\n",
    "print(\"Device name: {}\".format((x.device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "btf6G3Qg5Ild",
    "outputId": "2c950f35-8cec-48a6-9b3f-aea259554007"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(tf.executing_eagerly())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VURJE6TFJw6u"
   },
   "outputs": [],
   "source": [
    "#!wget http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamSeq01/CamSeq01.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BNNZu3EWKpwY",
    "outputId": "b284124c-337b-4e65-849e-bd3cda32c2b9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/mirap/tensorflowGUI/alpha/Multiclass-Semantic-Segmentation-CamVid'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8C3ZQAvcLJap"
   },
   "outputs": [],
   "source": [
    "#!mkdir data\n",
    "#!mkdir data/CamSeq01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o6FFQSbULYGI"
   },
   "outputs": [],
   "source": [
    "#!unzip CamSeq01.zip -d data/CamSeq01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5G4h2gJk5Ilk"
   },
   "source": [
    "## Data preparation - Importing, Cleaning and Creating structured directory "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z_V2LKfw5Ill"
   },
   "source": [
    "### Function to import and process frames and masks as tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jnbftYKV5Ilm"
   },
   "outputs": [],
   "source": [
    "def _read_to_tensor(fname, output_height=1120, output_width=1504, normalize_data=False):\n",
    "    '''Function to read images from given image file path, and provide resized images as tensors\n",
    "        Inputs: \n",
    "            fname - image file path\n",
    "            output_height - required output image height\n",
    "            output_width - required output image width\n",
    "            normalize_data - if True, normalize data to be centered around 0 (mean 0, range 0 to 1)\n",
    "        Output: Processed image tensors\n",
    "    '''\n",
    "    \n",
    "    # Read the image as a tensor\n",
    "    img_strings = tf.io.read_file(fname)\n",
    "    imgs_decoded = tf.image.decode_jpeg(img_strings)\n",
    "    \n",
    "    # Resize the image\n",
    "    output = tf.image.resize(imgs_decoded, [output_height, output_width])\n",
    "    \n",
    "    # Normalize if required\n",
    "    if normalize_data:\n",
    "        output = (output - 128) / 128\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tW2yaZz95Ilq"
   },
   "source": [
    "### Image directory and size parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oMHIUuNctJTz"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "\n",
    "## Try to remove tree; if failed show an error using try...except on screen\n",
    "#shutil.rmtree(\"data/CamSeq01/train_frames\")\n",
    "#shutil.rmtree(\"data/CamSeq01/train_masks\")\n",
    "#shutil.rmtree(\"data/CamSeq01/val_frames\")\n",
    "#shutil.rmtree(\"data/CamSeq01/val_masks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ca3Kut-X5Ilr"
   },
   "outputs": [],
   "source": [
    "img_dir = 'data/'\n",
    "\n",
    "# Required image dimensions\n",
    "output_height = 1120\n",
    "output_width = 1504"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UOreQ6fF5Ilv"
   },
   "source": [
    "### Reading frames and masks\n",
    "- Mask file names end in \"\\_L.png\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "5a5a1H2w5Ilx",
    "outputId": "b38ed282-c7d1-4119-ecf5-a81344ef09cf",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1522 frame files found in the provided directory.\n",
      "1522 mask files found in the provided directory.\n",
      "Completed importing 1522 frame images from the provided directory.\n",
      "Completed importing 1522 mask images from the provided directory.\n"
     ]
    }
   ],
   "source": [
    "def read_images(img_dir):\n",
    "    '''Function to get all image directories, read images and masks in separate tensors\n",
    "        Inputs: \n",
    "            img_dir - file directory\n",
    "        Outputs \n",
    "            frame_tensors, masks_tensors, frame files list, mask files list\n",
    "    '''\n",
    "    \n",
    "    # Get the file names list from provided directory\n",
    "    file_list = [f for f in os.listdir(img_dir) if os.path.isfile(os.path.join(img_dir, f))]\n",
    "    file_list.sort()\n",
    "    \n",
    "    # Separate frame and mask files lists, exclude unnecessary files\n",
    "    frames_list = [file for file in file_list if ('png' not in file) and ('txt' not in file)]\n",
    "    masks_list = [file for file in file_list if ('png' in file) and ('txt' not in file)]\n",
    "    \n",
    "    print('{} frame files found in the provided directory.'.format(len(frames_list)))\n",
    "    print('{} mask files found in the provided directory.'.format(len(masks_list)))\n",
    "    \n",
    "    # Create file paths from file names\n",
    "    frames_paths = [os.path.join(img_dir, fname) for fname in frames_list]\n",
    "    masks_paths = [os.path.join(img_dir, fname) for fname in masks_list]\n",
    "    \n",
    "    # Create dataset of tensors\n",
    "    frame_data = tf.data.Dataset.from_tensor_slices(frames_paths)\n",
    "    masks_data = tf.data.Dataset.from_tensor_slices(masks_paths)\n",
    "    \n",
    "    # Read images into the tensor dataset\n",
    "    frame_tensors = frame_data.map(_read_to_tensor)\n",
    "    masks_tensors = masks_data.map(_read_to_tensor)\n",
    "    \n",
    "    print('Completed importing {} frame images from the provided directory.'.format(len(frames_list)))\n",
    "    print('Completed importing {} mask images from the provided directory.'.format(len(masks_list)))\n",
    "    \n",
    "    return frame_tensors, masks_tensors, frames_list, masks_list\n",
    "\n",
    "frame_tensors, masks_tensors, frames_list, masks_list = read_images(img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1697
    },
    "colab_type": "code",
    "id": "l7jtlkv7uSFj",
    "outputId": "8733c74c-073c-4678-afb9-373e2f95b5aa"
   },
   "outputs": [],
   "source": [
    "#masks_list.sort()\n",
    "#print(len(masks_list))\n",
    "# masks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1697
    },
    "colab_type": "code",
    "id": "7YBUBme9uMu6",
    "outputId": "b68e84ff-6c95-4b30-ec34-6a2bb1d7e497"
   },
   "outputs": [],
   "source": [
    "#frames_list.sort()\n",
    "#print(len(frames_list))\n",
    "# frames_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0AyDQ5mt5Il3"
   },
   "source": [
    "### Displaying Images in the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Ky9ziLS5Il5",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Make an iterator to extract images from the tensor dataset\n",
    "\n",
    "frame_batches = tf.compat.v1.data.make_one_shot_iterator(frame_tensors)  # outside of TF Eager, we would use make_one_shot_iterator\n",
    "mask_batches = tf.compat.v1.data.make_one_shot_iterator(masks_tensors)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2093
    },
    "colab_type": "code",
    "id": "N51oaeudvlia",
    "outputId": "f9c6bc78-5d08-478a-d6ed-7f9eb76b4654"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nn_images_to_show = 5\\n\\nfor i in range(n_images_to_show):\\n    \\n    # Get the next image from iterator\\n    frame = frame_batches.next().numpy().astype(np.uint8)\\n    mask = mask_batches.next().numpy().astype(np.uint8)\\n    \\n    #Plot the corresponding frames and masks\\n    fig = plt.figure(figsize=(20,7))\\n    fig.add_subplot(1,2,1)\\n    plt.grid(b=None)\\n    plt.imshow(frame)\\n    fig.add_subplot(1,2,2)\\n    plt.grid(b=None)\\n    plt.imshow(mask)\\n    plt.show()\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "n_images_to_show = 5\n",
    "\n",
    "for i in range(n_images_to_show):\n",
    "    \n",
    "    # Get the next image from iterator\n",
    "    frame = frame_batches.next().numpy().astype(np.uint8)\n",
    "    mask = mask_batches.next().numpy().astype(np.uint8)\n",
    "    \n",
    "    #Plot the corresponding frames and masks\n",
    "    fig = plt.figure(figsize=(20,7))\n",
    "    fig.add_subplot(1,2,1)\n",
    "    plt.grid(b=None)\n",
    "    plt.imshow(frame)\n",
    "    fig.add_subplot(1,2,2)\n",
    "    plt.grid(b=None)\n",
    "    plt.imshow(mask)\n",
    "    plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kS6gUJqG5Il9"
   },
   "source": [
    "### Creating folder structure common for Computer Vision problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ccoh2FNR5Il_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 17] File exists: 'data/CamSeq01/train_frames/train'\n",
      "[Errno 17] File exists: 'data/CamSeq01/train_masks/train'\n",
      "[Errno 17] File exists: 'data/CamSeq01/val_frames/val'\n",
      "[Errno 17] File exists: 'data/CamSeq01/val_masks/val'\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = 'data/CamSeq01/'\n",
    "\n",
    "# Create folders to hold images and masks\n",
    "\n",
    "folders = ['train_frames/train', 'train_masks/train', 'val_frames/val', 'val_masks/val']\n",
    "\n",
    "\n",
    "for folder in folders:\n",
    "    try:\n",
    "        os.makedirs(DATA_PATH + folder)\n",
    "    except Exception as e: print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iq7ZIz9_5ImC"
   },
   "source": [
    "### Saving frames and masks to correct directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "xHfnxEUX5ImD",
    "outputId": "23e59d8c-dfca-4e7c-e7ae-a6a24dae2926"
   },
   "outputs": [],
   "source": [
    "def generate_image_folder_structure(frames, masks, frames_list, masks_list):\n",
    "    '''Function to save images in the appropriate folder directories \n",
    "        Inputs: \n",
    "            frames - frame tensor dataset\n",
    "            masks - mask tensor dataset\n",
    "            frames_list - frame file paths\n",
    "            masks_list - mask file paths\n",
    "    '''\n",
    "    #Create iterators for frames and masks\n",
    "    frame_batches = tf.compat.v1.data.make_one_shot_iterator(frames)  # outside of TF Eager, we would use make_one_shot_iterator\n",
    "    mask_batches = tf.compat.v1.data.make_one_shot_iterator(masks)\n",
    "    \n",
    "    #Iterate over the train images while saving the frames and masks in appropriate folders\n",
    "    dir_name='train'\n",
    "    for file in zip(frames_list[:-round(0.2*len(frames_list))],masks_list[:-round(0.2*len(masks_list))]):\n",
    "        \n",
    "        \n",
    "        #Convert tensors to numpy arrays\n",
    "        frame = frame_batches.next().numpy().astype(np.uint8)\n",
    "        mask = mask_batches.next().numpy().astype(np.uint8)\n",
    "        \n",
    "        #Convert numpy arrays to images\n",
    "        frame = Image.fromarray(frame)\n",
    "        mask = Image.fromarray(mask)\n",
    "        \n",
    "        #Save frames and masks to correct directories\n",
    "        frame.save(DATA_PATH+'{}_frames/{}'.format(dir_name,dir_name)+'/'+file[0])\n",
    "        mask.save(DATA_PATH+'{}_masks/{}'.format(dir_name,dir_name)+'/'+file[1])\n",
    "    \n",
    "    #Iterate over the val images while saving the frames and masks in appropriate folders\n",
    "    dir_name='val'\n",
    "    for file in zip(frames_list[-round(0.2*len(frames_list)):],masks_list[-round(0.2*len(masks_list)):]):\n",
    "        \n",
    "        \n",
    "        #Convert tensors to numpy arrays\n",
    "        frame = frame_batches.next().numpy().astype(np.uint8)\n",
    "        mask = mask_batches.next().numpy().astype(np.uint8)\n",
    "        \n",
    "        #Convert numpy arrays to images\n",
    "        frame = Image.fromarray(frame)\n",
    "        mask = Image.fromarray(mask)\n",
    "        \n",
    "        #Save frames and masks to correct directories\n",
    "        frame.save(DATA_PATH+'{}_frames/{}'.format(dir_name,dir_name)+'/'+file[0])\n",
    "        mask.save(DATA_PATH+'{}_masks/{}'.format(dir_name,dir_name)+'/'+file[1])\n",
    "    \n",
    "    print(\"Saved {} frames to directory {}\".format(len(frames_list),DATA_PATH))\n",
    "    print(\"Saved {} masks to directory {}\".format(len(masks_list),DATA_PATH))\n",
    "    \n",
    "## If you already devided it manually, you do not need to run this.   \n",
    "#generate_image_folder_structure(frame_tensors, masks_tensors, frames_list, masks_list)\n",
    "\n",
    "#generate_image_folder_structure(train_frames, train_masks, val_files, 'val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LCJwUktZ5ImI"
   },
   "source": [
    "## Extract Target Class definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IAndSyXG5ImJ"
   },
   "source": [
    "### Function to parse the file \"label_colors.txt\" which contains the class definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "swcxQC-15ImK"
   },
   "outputs": [],
   "source": [
    "def parse_code(l):\n",
    "    '''Function to parse lines in a text file, returns separated elements (label codes and names in this case)\n",
    "    '''\n",
    "    if len(l.strip().split(\"\\t\")) == 2:\n",
    "        a, b = l.strip().split(\"\\t\")\n",
    "        return tuple(int(i) for i in a.split(' ')), b\n",
    "    else:\n",
    "        a, b, c = l.strip().split(\"\\t\")\n",
    "        return tuple(int(i) for i in a.split(' ')), c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FQJCFIe-5ImQ"
   },
   "source": [
    "### Parse and extract label names and codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "s0FF09JP5ImR",
    "outputId": "61f65da3-0201-4c02-ac6e-602aec99be62"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(176, 112, 32),\n",
       "  (176, 240, 32),\n",
       "  (48, 112, 32),\n",
       "  (112, 112, 32),\n",
       "  (48, 240, 32)],\n",
       " ['Cortical subplate',\n",
       "  'Hypothalamus',\n",
       "  'Cerebral nuclei',\n",
       "  'auditory area',\n",
       "  'Hippocampal formation'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_codes = [(176,112,32),(176,240,32),(48,112,32),(112,112,32),(48,240,32),(176,240,160),(176,112,160),(48,112,160),(48,240,160),(5,5,4),(5,5,4),(5,5,4),(5,5,4),(5,5,4),(5,5,4),(5,5,4),(5,5,4),(5,5,4),(5,5,4),(5,5,4),(5,5,4),(5,5,4),(5,5,4),(5,5,4),(5,5,4),(5,5,4),(5,5,4),(5,5,4),(5,5,4),(5,5,4),(5,5,4),(5,5,4),]\n",
    "label_names = ['Cortical subplate','Hypothalamus','Cerebral nuclei','auditory area','Hippocampal formation','Thalamus','Retrosplenial area','Olfactory area','Somatosensory area','dump','dump','dump','dump','dump','dump','dump','dump','dump','dump','dump','dump','dump','dump','dump','dump','dump','dump','dump','dump','dump','dump','dump']\n",
    "label_codes[:5], label_names[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1092
    },
    "colab_type": "code",
    "id": "lqbePydU5ImX",
    "outputId": "2633edbd-e3db-4107-a70a-6f8e221fea16"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(176, 112, 32),\n",
       "  (176, 240, 32),\n",
       "  (48, 112, 32),\n",
       "  (112, 112, 32),\n",
       "  (48, 240, 32),\n",
       "  (176, 240, 160),\n",
       "  (176, 112, 160),\n",
       "  (48, 112, 160),\n",
       "  (48, 240, 160),\n",
       "  (5, 5, 4),\n",
       "  (5, 5, 4),\n",
       "  (5, 5, 4),\n",
       "  (5, 5, 4),\n",
       "  (5, 5, 4),\n",
       "  (5, 5, 4),\n",
       "  (5, 5, 4),\n",
       "  (5, 5, 4),\n",
       "  (5, 5, 4),\n",
       "  (5, 5, 4),\n",
       "  (5, 5, 4),\n",
       "  (5, 5, 4),\n",
       "  (5, 5, 4),\n",
       "  (5, 5, 4),\n",
       "  (5, 5, 4),\n",
       "  (5, 5, 4),\n",
       "  (5, 5, 4),\n",
       "  (5, 5, 4),\n",
       "  (5, 5, 4),\n",
       "  (5, 5, 4),\n",
       "  (5, 5, 4),\n",
       "  (5, 5, 4),\n",
       "  (5, 5, 4)],\n",
       " ['Cortical subplate',\n",
       "  'Hypothalamus',\n",
       "  'Cerebral nuclei',\n",
       "  'auditory area',\n",
       "  'Hippocampal formation',\n",
       "  'Thalamus',\n",
       "  'Retrosplenial area',\n",
       "  'Olfactory area',\n",
       "  'Somatosensory area',\n",
       "  'dump',\n",
       "  'dump',\n",
       "  'dump',\n",
       "  'dump',\n",
       "  'dump',\n",
       "  'dump',\n",
       "  'dump',\n",
       "  'dump',\n",
       "  'dump',\n",
       "  'dump',\n",
       "  'dump',\n",
       "  'dump',\n",
       "  'dump',\n",
       "  'dump',\n",
       "  'dump',\n",
       "  'dump',\n",
       "  'dump',\n",
       "  'dump',\n",
       "  'dump',\n",
       "  'dump',\n",
       "  'dump',\n",
       "  'dump',\n",
       "  'dump'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_codes, label_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qw7yYAO_5Imb"
   },
   "source": [
    "### Create useful label and code conversion dictionaries\n",
    "_These will be used for:_\n",
    "- One hot encoding the mask labels for model training\n",
    "- Decoding the predicted labels for interpretation and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "raQmHVsB5Imb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "code2id = {v:k for k,v in enumerate(label_codes)}\n",
    "id2code = {k:v for k,v in enumerate(label_codes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uz_5a3IX5Img"
   },
   "outputs": [],
   "source": [
    "name2id = {v:k for k,v in enumerate(label_names)}\n",
    "id2name = {k:v for k,v in enumerate(label_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 554
    },
    "colab_type": "code",
    "id": "2nPNoP_S5Imi",
    "outputId": "0cb443b9-d93d-4d50-bbfa-a9778ff71d15"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: (176, 112, 32),\n",
       " 1: (176, 240, 32),\n",
       " 2: (48, 112, 32),\n",
       " 3: (112, 112, 32),\n",
       " 4: (48, 240, 32),\n",
       " 5: (176, 240, 160),\n",
       " 6: (176, 112, 160),\n",
       " 7: (48, 112, 160),\n",
       " 8: (48, 240, 160),\n",
       " 9: (5, 5, 4),\n",
       " 10: (5, 5, 4),\n",
       " 11: (5, 5, 4),\n",
       " 12: (5, 5, 4),\n",
       " 13: (5, 5, 4),\n",
       " 14: (5, 5, 4),\n",
       " 15: (5, 5, 4),\n",
       " 16: (5, 5, 4),\n",
       " 17: (5, 5, 4),\n",
       " 18: (5, 5, 4),\n",
       " 19: (5, 5, 4),\n",
       " 20: (5, 5, 4),\n",
       " 21: (5, 5, 4),\n",
       " 22: (5, 5, 4),\n",
       " 23: (5, 5, 4),\n",
       " 24: (5, 5, 4),\n",
       " 25: (5, 5, 4),\n",
       " 26: (5, 5, 4),\n",
       " 27: (5, 5, 4),\n",
       " 28: (5, 5, 4),\n",
       " 29: (5, 5, 4),\n",
       " 30: (5, 5, 4),\n",
       " 31: (5, 5, 4)}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 554
    },
    "colab_type": "code",
    "id": "vtfrmJc05Imo",
    "outputId": "4517846a-276b-4b42-a15d-7e348b13e0f5",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Cortical subplate',\n",
       " 1: 'Hypothalamus',\n",
       " 2: 'Cerebral nuclei',\n",
       " 3: 'auditory area',\n",
       " 4: 'Hippocampal formation',\n",
       " 5: 'Thalamus',\n",
       " 6: 'Retrosplenial area',\n",
       " 7: 'Olfactory area',\n",
       " 8: 'Somatosensory area',\n",
       " 9: 'dump',\n",
       " 10: 'dump',\n",
       " 11: 'dump',\n",
       " 12: 'dump',\n",
       " 13: 'dump',\n",
       " 14: 'dump',\n",
       " 15: 'dump',\n",
       " 16: 'dump',\n",
       " 17: 'dump',\n",
       " 18: 'dump',\n",
       " 19: 'dump',\n",
       " 20: 'dump',\n",
       " 21: 'dump',\n",
       " 22: 'dump',\n",
       " 23: 'dump',\n",
       " 24: 'dump',\n",
       " 25: 'dump',\n",
       " 26: 'dump',\n",
       " 27: 'dump',\n",
       " 28: 'dump',\n",
       " 29: 'dump',\n",
       " 30: 'dump',\n",
       " 31: 'dump'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uss876Ym5Ims"
   },
   "source": [
    "### Define functions for one hot encoding rgb labels, and decoding encoded predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vDyT3aVR5Imt"
   },
   "outputs": [],
   "source": [
    "def rgb_to_onehot(rgb_image, colormap = id2code):\n",
    "    '''Function to one hot encode RGB mask labels\n",
    "        Inputs: \n",
    "            rgb_image - image matrix (eg. 256 x 256 x 3 dimension numpy ndarray)\n",
    "            colormap - dictionary of color to label id\n",
    "        Output: One hot encoded image of dimensions (height x width x num_classes) where num_classes = len(colormap)\n",
    "    '''\n",
    "    num_classes = len(colormap)\n",
    "    shape = rgb_image.shape[:2]+(num_classes,)\n",
    "    encoded_image = np.zeros( shape, dtype=np.int8 )\n",
    "    for i, cls in enumerate(colormap):\n",
    "        encoded_image[:,:,i] = np.all(rgb_image.reshape( (-1,3) ) == colormap[i], axis=1).reshape(shape[:2])\n",
    "    return encoded_image\n",
    "\n",
    "\n",
    "def onehot_to_rgb(onehot, colormap = id2code):\n",
    "    '''Function to decode encoded mask labels\n",
    "        Inputs: \n",
    "            onehot - one hot encoded image matrix (height x width x num_classes)\n",
    "            colormap - dictionary of color to label id\n",
    "        Output: Decoded RGB image (height x width x 3) \n",
    "    '''\n",
    "    single_layer = np.argmax(onehot, axis=-1)\n",
    "    output = np.zeros( onehot.shape[:2]+(3,) )\n",
    "    for k in colormap.keys():\n",
    "        output[single_layer==k] = colormap[k]\n",
    "    return np.uint8(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tSXz_nwP5Imw"
   },
   "source": [
    "# Creating custom Image data generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JksmH6fN5Imx"
   },
   "source": [
    "### Defining data generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NeWu4wmh5Imy"
   },
   "outputs": [],
   "source": [
    "# Normalizing only frame images, since masks contain label info\n",
    "data_gen_args = dict(rescale=1./255)\n",
    "mask_gen_args = dict()\n",
    "\n",
    "train_frames_datagen = ImageDataGenerator(**data_gen_args)\n",
    "train_masks_datagen = ImageDataGenerator(**mask_gen_args)\n",
    "val_frames_datagen = ImageDataGenerator(**data_gen_args)\n",
    "val_masks_datagen = ImageDataGenerator(**mask_gen_args)\n",
    "test_frames_datagen = ImageDataGenerator(**data_gen_args)\n",
    "test_masks_datagen = ImageDataGenerator(**mask_gen_args)\n",
    "\n",
    "# Seed defined for aligning images and their masks\n",
    "seed = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1OR6bZ-n5Im1"
   },
   "source": [
    "### Custom image data generators for creating batches of frames and masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TeP5Wqr75Im2"
   },
   "outputs": [],
   "source": [
    "def TrainAugmentGenerator(seed = 1, batch_size = 5):\n",
    "    '''Train Image data generator\n",
    "        Inputs: \n",
    "            seed - seed provided to the flow_from_directory function to ensure aligned data flow\n",
    "            batch_size - number of images to import at a time\n",
    "        Output: Decoded RGB image (height x width x 3) \n",
    "    '''\n",
    "    train_image_generator = train_frames_datagen.flow_from_directory(\n",
    "    DATA_PATH + 'train_frames/',\n",
    "    batch_size = batch_size, seed = seed, target_size = (224, 224))\n",
    "\n",
    "    train_mask_generator = train_masks_datagen.flow_from_directory(\n",
    "    DATA_PATH + 'train_masks/',\n",
    "    batch_size = batch_size, seed = seed, target_size = (224, 224))\n",
    "\n",
    "    while True:\n",
    "        X1i = train_image_generator.next()\n",
    "        X2i = train_mask_generator.next()\n",
    "        \n",
    "        #One hot encoding RGB images\n",
    "        mask_encoded = [rgb_to_onehot(X2i[0][x,:,:,:], id2code) for x in range(X2i[0].shape[0])]\n",
    "        \n",
    "        yield X1i[0], np.asarray(mask_encoded)\n",
    "\n",
    "def ValAugmentGenerator(seed = 1, batch_size = 5):\n",
    "    '''Validation Image data generator\n",
    "        Inputs: \n",
    "            seed - seed provided to the flow_from_directory function to ensure aligned data flow\n",
    "            batch_size - number of images to import at a time\n",
    "        Output: Decoded RGB image (height x width x 3) \n",
    "    '''\n",
    "    val_image_generator = val_frames_datagen.flow_from_directory(\n",
    "    DATA_PATH + 'val_frames/',\n",
    "    batch_size = batch_size, seed = seed, target_size = (224, 224))\n",
    "\n",
    "\n",
    "    val_mask_generator = val_masks_datagen.flow_from_directory(\n",
    "    DATA_PATH + 'val_masks/',\n",
    "    batch_size = batch_size, seed = seed, target_size = (224, 224))\n",
    "\n",
    "\n",
    "    while True:\n",
    "        X1i = val_image_generator.next()\n",
    "        X2i = val_mask_generator.next()\n",
    "        \n",
    "        #One hot encoding RGB images\n",
    "        mask_encoded = [rgb_to_onehot(X2i[0][x,:,:,:], id2code) for x in range(X2i[0].shape[0])]\n",
    "        \n",
    "        yield X1i[0], np.asarray(mask_encoded)\n",
    "def TestAugmentGenerator(seed = 1, batch_size = 10):\n",
    "    '''test Image data generator\n",
    "        Inputs: \n",
    "            seed - seed provided to the flow_from_directory function to ensure aligned data flow\n",
    "            batch_size - number of images to import at a time\n",
    "        Output: Decoded RGB image (height x width x 3) \n",
    "    '''\n",
    "    test_image_generator = test_frames_datagen.flow_from_directory(\n",
    "    DATA_PATH + 'test_frames/',\n",
    "    batch_size = batch_size, seed = seed, target_size = (224, 224))\n",
    "\n",
    "\n",
    "    test_mask_generator = test_masks_datagen.flow_from_directory(\n",
    "    DATA_PATH + 'test_masks/',\n",
    "    batch_size = batch_size, seed = seed, target_size = (224, 224))\n",
    "\n",
    "\n",
    "    while True:\n",
    "        X1i = test_image_generator.next()\n",
    "        X2i = test_mask_generator.next()\n",
    "        \n",
    "        #One hot encoding RGB images\n",
    "        mask_encoded = [rgb_to_onehot(X2i[0][x,:,:,:], id2code) for x in range(X2i[0].shape[0])]\n",
    "        \n",
    "        yield X1i[0], np.asarray(mask_encoded)        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RcM6HS4B5Im6"
   },
   "source": [
    "# Defining VGG-16 SegNet model for semantic segmentation\n",
    "<b>SegNet Architecture</b>\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "        \n",
    "        <img src=\"https://www.researchgate.net/profile/Fang_Liu67/publication/318638568/figure/fig1/AS:519332650119168@1500829960512/An-illustration-of-the-SegNet-CNN-architecture-SegNet-contains-an-encoder-network-and-a.png\" alt=\"U-Net\" height=\"600\" width=\"800\">\n",
    "</div>\n",
    "\n",
    "[Picture Credits](https://www.researchgate.net/figure/An-illustration-of-the-SegNet-CNN-architecture-SegNet-contains-an-encoder-network-and-a_fig1_318638568)<br><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lhg8CrPP5Im6"
   },
   "outputs": [],
   "source": [
    "# from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NL0lO3H25Im9"
   },
   "outputs": [],
   "source": [
    "# from tqdm import tqdm_gui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to create SegNet model using VGG-16 pre-trained weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O7FDyfR9RTcI"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "\n",
    "import os\n",
    "VGG_Weights_path = \"pretrained_weights/vgg16_weights_tf_dim_ordering_tf_kernels_2.h5\"\n",
    "\n",
    "\n",
    "def VGGSegnet( n_classes ,  input_height=224, input_width=224 , vgg_level=3):\n",
    "\n",
    "    img_input = Input(shape=(input_height,input_width,3))\n",
    "\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1', data_format='channels_last' )(img_input)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2', data_format='channels_last' )(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool', data_format='channels_last' )(x)\n",
    "    f1 = x\n",
    "    # Block 2\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1', data_format='channels_last' )(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2', data_format='channels_last' )(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool', data_format='channels_last' )(x)\n",
    "    f2 = x\n",
    "\n",
    "    # Block 3\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1', data_format='channels_last' )(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2', data_format='channels_last' )(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3', data_format='channels_last' )(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool', data_format='channels_last' )(x)\n",
    "    f3 = x\n",
    "\n",
    "    # Block 4\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1', data_format='channels_last' )(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2', data_format='channels_last' )(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3', data_format='channels_last' )(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool', data_format='channels_last' )(x)\n",
    "    f4 = x\n",
    "\n",
    "    # Block 5\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1', data_format='channels_last' )(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2', data_format='channels_last' )(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3', data_format='channels_last' )(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool', data_format='channels_last' )(x)\n",
    "    f5 = x\n",
    "\n",
    "    x = Flatten(name='flatten')(x)\n",
    "    x = Dense(4096, activation='relu', name='fc1')(x)\n",
    "    x = Dense(4096, activation='relu', name='fc2')(x)\n",
    "    x = Dense( 1000 , activation='softmax', name='predictions')(x)\n",
    "\n",
    "    vgg  = Model(  img_input , x  )\n",
    "    # vgg.load_weights(VGG_Weights_path)\n",
    "\n",
    "    levels = [f1 , f2 , f3 , f4 , f5 ]\n",
    "\n",
    "    o = levels[ vgg_level ]\n",
    "\n",
    "    o = ( ZeroPadding2D( (1,1) , data_format='channels_last' ))(o)\n",
    "    o = ( Conv2D(512, (3, 3), padding='valid', data_format='channels_last'))(o)\n",
    "    o = ( BatchNormalization())(o)\n",
    "    \n",
    "    o = ( UpSampling2D( (2,2), data_format='channels_last'))(o)\n",
    "    o = ( ZeroPadding2D( (1,1), data_format='channels_last'))(o)\n",
    "    o = ( Conv2D( 512, (3, 3), padding='valid', data_format='channels_last'))(o)\n",
    "    o = ( BatchNormalization())(o)\n",
    "    \n",
    "    o = ( UpSampling2D( (2,2), data_format='channels_last'))(o)\n",
    "    o = ( ZeroPadding2D( (1,1), data_format='channels_last'))(o)\n",
    "    o = ( Conv2D( 256, (3, 3), padding='valid', data_format='channels_last'))(o)\n",
    "    o = ( BatchNormalization())(o)\n",
    "\n",
    "    o = ( UpSampling2D((2,2)  , data_format='channels_last' ) )(o)\n",
    "    o = ( ZeroPadding2D((1,1) , data_format='channels_last' ))(o)\n",
    "    o = ( Conv2D( 128 , (3, 3), padding='valid' , data_format='channels_last' ))(o)\n",
    "    o = ( BatchNormalization())(o)\n",
    "\n",
    "    o = ( UpSampling2D((2,2)  , data_format='channels_last' ))(o)\n",
    "    o = ( ZeroPadding2D((1,1)  , data_format='channels_last' ))(o)\n",
    "    o = ( Conv2D( 64 , (3, 3), padding='valid'  , data_format='channels_last' ))(o)\n",
    "    o = ( BatchNormalization())(o)\n",
    "    \n",
    "    \n",
    "\n",
    "    o =  Conv2D( n_classes , (3, 3) , padding='same', data_format='channels_last' )( o )\n",
    "    #o_shape = Model(img_input , o ).output_shape\n",
    "    #outputHeight = o_shape[2]\n",
    "    #outputWidth = o_shape[3]\n",
    "\n",
    "    #o = (Reshape((  -1  , outputHeight*outputWidth   )))(o)\n",
    "    #o = (Permute((2, 1)))(o)\n",
    "    o = (Activation('softmax'))(o)\n",
    "    model = Model( img_input , o )\n",
    "    #model.outputWidth = outputWidth\n",
    "    #model.outputHeight = outputHeight\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CVZ4GjjrSIA_"
   },
   "outputs": [],
   "source": [
    "#os.getcwd()\n",
    "#!mkdir pretrained_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uFmjjMx-SZWC"
   },
   "outputs": [],
   "source": [
    "#!wget -O pretrained_weights/vgg16_weights_tf_dim_ordering_tf_kernels.h5  https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3UoCXGTySEF7"
   },
   "outputs": [],
   "source": [
    "model = VGGSegnet( 32  , vgg_level=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1361
    },
    "colab_type": "code",
    "id": "01vKF8e85InG",
    "outputId": "6bd50cf7-3274-4b49-9323-8059a9b5e417"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2 (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "up_sampling2d (UpSampling2D) (None, 28, 28, 512)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, 30, 30, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 28, 28, 512)       2048      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 56, 56, 512)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPaddin (None, 58, 58, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 56, 56, 256)       1179904   \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 56, 56, 256)       1024      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 112, 112, 256)     0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPaddin (None, 114, 114, 256)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 112, 112, 128)     295040    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 112, 112, 128)     512       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 224, 224, 128)     0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPaddin (None, 226, 226, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 224, 224, 64)      73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 224, 224, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 224, 224, 32)      18464     \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 224, 224, 32)      0         \n",
      "=================================================================\n",
      "Total params: 13,927,968\n",
      "Trainable params: 13,925,024\n",
      "Non-trainable params: 2,944\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OQe0Ult35InP"
   },
   "source": [
    "## Defining dice co-efficients for model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p_AtXHM65InQ"
   },
   "outputs": [],
   "source": [
    "# Ref: salehi17, \"Twersky loss function for image segmentation using 3D FCDN\"\n",
    "# -> the score is computed for each class separately and then summed\n",
    "# alpha=beta=0.5 : dice coefficient\n",
    "# alpha=beta=1   : tanimoto coefficient (also known as jaccard)\n",
    "# alpha+beta=1   : produces set of F*-scores\n",
    "# implemented by E. Moebel, 06/04/18\n",
    "def tversky_loss(y_true, y_pred):\n",
    "    alpha = 0.5\n",
    "    beta  = 0.5\n",
    "    \n",
    "    ones = K.ones(K.shape(y_true))\n",
    "    p0 = y_pred      # proba that voxels are class i\n",
    "    p1 = ones-y_pred # proba that voxels are not class i\n",
    "    g0 = y_true\n",
    "    g1 = ones-y_true\n",
    "    \n",
    "    num = K.sum(p0*g0, (0,1,2,3))\n",
    "    den = num + alpha*K.sum(p0*g1,(0,1,2,3)) + beta*K.sum(p1*g0,(0,1,2,3))\n",
    "    \n",
    "    T = K.sum(num/den) # when summing over classes, T has dynamic range [0 Ncl]\n",
    "    \n",
    "    Ncl = K.cast(K.shape(y_true)[-1], 'float32')\n",
    "    return Ncl-T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TEKJIXGN5InT"
   },
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f*y_true_f) + K.sum(y_pred_f*y_pred_f) + smooth)\n",
    "\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1.-dice_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (intersection + smooth) / (K.sum(y_true_f*y_true_f) + K.sum(y_pred_f*y_pred_f) - intersection + smooth)\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    y_true = K.ones_like(y_true) \n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    all_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    \n",
    "    recall = true_positives / (all_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    y_true = K.ones_like(y_true) \n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    \n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    precision_m = precision(y_true, y_pred)\n",
    "    recall_m = recall(y_true, y_pred)\n",
    "    return 2*((precision_m*recall_m)/(precision_m+recall_m+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zEiX00HV5InY"
   },
   "outputs": [],
   "source": [
    "smooth = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "moFPMWwd5Inb"
   },
   "source": [
    "## Compiling model\n",
    "- Using categorical crossentropy loss since labels have been one hot encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1F12FjDY5Inc"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics=[tversky_loss,dice_coef,iou,recall,precision,f1_score,'accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1361
    },
    "colab_type": "code",
    "id": "L8xTxM5z5Inf",
    "outputId": "1aa99844-32af-4622-ccc9-082a135c388d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2 (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "up_sampling2d (UpSampling2D) (None, 28, 28, 512)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, 30, 30, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 28, 28, 512)       2048      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 56, 56, 512)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPaddin (None, 58, 58, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 56, 56, 256)       1179904   \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 56, 56, 256)       1024      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 112, 112, 256)     0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPaddin (None, 114, 114, 256)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 112, 112, 128)     295040    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 112, 112, 128)     512       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 224, 224, 128)     0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPaddin (None, 226, 226, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 224, 224, 64)      73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 224, 224, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 224, 224, 32)      18464     \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 224, 224, 32)      0         \n",
      "=================================================================\n",
      "Total params: 13,927,968\n",
      "Trainable params: 13,925,024\n",
      "Non-trainable params: 2,944\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TtRekxsH5Inl"
   },
   "outputs": [],
   "source": [
    "model.load_weights(\"camvid_model_vgg16_segnet_v1_aug.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lr_J6stp5Ino"
   },
   "source": [
    "## Define model callback settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "crvxCVqF5Inp"
   },
   "outputs": [],
   "source": [
    "tb = TensorBoard(log_dir='logs', write_graph=True)\n",
    "mc = ModelCheckpoint(mode='max', filepath='camvid_model_vgg16_segnet_v1_100e_checkpoint.h5', monitor='accuracy', save_best_only='True', save_weights_only='True', verbose=1)\n",
    "es = EarlyStopping(mode='min', monitor='val_loss', patience=50, verbose=1)\n",
    "callbacks = [tb, mc, es]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9UXxe0VB5Inr"
   },
   "source": [
    "## Train and save the U-Net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "T_jFkdpN5Inr",
    "outputId": "b0ca41d6-9267-42c2-9aa9-b522bdaad8aa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "244.0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 5\n",
    "steps_per_epoch = np.ceil(float(len(frames_list) - round(0.2*len(frames_list))) / float(batch_size))\n",
    "steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mLjqyOTA5Inw",
    "outputId": "bd7a5d3e-a2d7-4981-ccd9-01e675a7c412"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60.8"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_steps = (float((round(0.2*len(frames_list)))) / float(batch_size))\n",
    "validation_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-xSAqruW5In6"
   },
   "outputs": [],
   "source": [
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 7849
    },
    "colab_type": "code",
    "id": "O6RBEiX55IoM",
    "outputId": "c5937e35-3c64-4958-caaa-9c1e0652612a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1218 images belonging to 1 classes.\n",
      "Found 1218 images belonging to 1 classes.\n",
      "WARNING:tensorflow:From /home/mirap/anaconda3/envs/alpha/lib/python3.5/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Found 294 images belonging to 1 classes.\n",
      "Found 294 images belonging to 1 classes.\n",
      "WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n",
      "1/1 [==============================] - 46s 46s/step - loss: 0.0238 - tversky_loss: 31.1649 - dice_coef: 0.8477 - iou: 0.7357 - recall: 0.0312 - precision: 1.0000 - f1_score: 0.0604 - acc: 0.9535 - val_loss: 0.3826 - val_tversky_loss: 31.2193 - val_dice_coef: 0.7947 - val_iou: 0.6605 - val_recall: 0.0312 - val_precision: 1.0000 - val_f1_score: 0.0605 - val_acc: 0.9047\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "\n",
    "batch_size = 5\n",
    "result = model.fit_generator(TrainAugmentGenerator(), steps_per_epoch=1 ,\n",
    "                validation_data = ValAugmentGenerator(), \n",
    "                validation_steps = validation_steps, \n",
    "                epochs=1, \n",
    "                callbacks=callbacks, \n",
    "                verbose=1)\n",
    "model.save_weights(\"camvid_model_vgg16_segnet_v1_100etest9.h5\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "usVXAHBB5IoQ"
   },
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M1jHDmdc5IoR"
   },
   "source": [
    "### Model evaluation historical plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 516
    },
    "colab_type": "code",
    "id": "ULtl_7MB5IoT",
    "outputId": "10aac0c9-f3b6-4225-ac06-6bb0100c0bba"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/alpha/lib/python3.5/site-packages/tensorflow/python/keras/api/_v1/keras/layers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training Accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train_accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val_accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"val_accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'accuracy'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get actual number of epochs model was trained for\n",
    "N = len(result.history['loss'])\n",
    "\n",
    "#Plot the model evaluation history\n",
    "plt.style.use(\"ggplot\")\n",
    "fig = plt.figure(figsize=(20,8))\n",
    "\n",
    "fig.add_subplot(1,2,1)\n",
    "plt.title(\"Training Loss\")\n",
    "plt.plot(np.arange(0, N), result.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, N), result.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "fig.add_subplot(1,2,2)\n",
    "plt.title(\"Training Accuracy\")\n",
    "plt.plot(np.arange(0, N), result.history[\"accuracy\"], label=\"train_accuracy\")\n",
    "plt.plot(np.arange(0, N), result.history[\"val_accuracy\"], label=\"val_accuracy\")\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K6grhRPC5Iob"
   },
   "source": [
    "### Extract and display model frame, prediction and mask batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I-jMo62u5IoX"
   },
   "outputs": [],
   "source": [
    "# training_gen = TrainAugmentGenerator()\n",
    "testing_gen = TestAugmentGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "NxCJnbik5Ioc",
    "outputId": "485bc8a5-5c1c-4d6d-ff86-1fe894decd24"
   },
   "outputs": [],
   "source": [
    "\n",
    "batch_img,batch_mask = next(testing_gen)\n",
    "pred_all= model.predict(batch_img)\n",
    "np.shape(pred_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1897
    },
    "colab_type": "code",
    "id": "q8WCLKd65Iog",
    "outputId": "172dcc11-8eb8-4dbc-dfbf-c80cab2c6cd2",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(0,np.shape(pred_all)[0]):\n",
    "    \n",
    "    fig = plt.figure(figsize=(20,8))\n",
    "    \n",
    "    ax1 = fig.add_subplot(1,3,1)\n",
    "    ax1.imshow(batch_img[i])\n",
    "    ax1.title.set_text('Actual frame')\n",
    "    ax1.grid(b=None)\n",
    "    \n",
    "    \n",
    "    ax2 = fig.add_subplot(1,3,2)\n",
    "    ax2.set_title('Ground truth labels')\n",
    "    ax2.imshow(onehot_to_rgb(batch_mask[i],id2code))\n",
    "    ax2.grid(b=None)\n",
    "    \n",
    "    ax3 = fig.add_subplot(1,3,3)\n",
    "    ax3.set_title('Predicted labels')\n",
    "    ax3.imshow(onehot_to_rgb(pred_all[i],id2code))\n",
    "    ax3.grid(b=None)\n",
    "    \n",
    "    plt.savefig(\"SegNet_result_\"+str(i))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5x-e433P5Iol"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3kbS3VnQ5Ion"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Multiclass Semantic Segmentation using VGG-16 SegNet.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
